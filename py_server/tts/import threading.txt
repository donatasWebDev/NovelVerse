import threading
import time
import logging
import uuid
import queue

# Configure basic logging for better visibility
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(threadName)s - %(message)s')

# --- 1. TTSPipeline (Simulated) ---
# This class represents your actual TTS model interaction.
# It checks the task-specific cancellation event.
class TTSPipeline:
    def __init__(self, worker_id):
        self.worker_id = worker_id
        logging.info(f"TTSPipeline {self.worker_id}: Initialized.")

    def generate_audio_chunks(self, text_data: dict, task_cancellation_event: threading.Event):
        """
        Simulates generating audio chunks from text.
        Crucially, it checks the task_cancellation_event.
        """
        socket_id = text_data.get('socket_id', 'unknown')
        text_content = text_data.get('data', '')
        logging.info(f"TTSPipeline {self.worker_id}: Starting generation for socket {socket_id} (text: '{text_content[:20]}')...")

        total_chunks = 15 # Simulate a long generation process
        for i in range(total_chunks):
            # *** CORE CANCELLATION CHECK ***
            if task_cancellation_event.is_set():
                logging.warning(f"TTSPipeline {self.worker_id}: Cancellation detected for socket {socket_id}! Aborting generation.")
                break # Exit the generation loop immediately

            time.sleep(0.05) # Simulate work per chunk (e.g., calling actual TTS model)
            audio_chunk = f"chunk_{i+1}_of_'{text_content[:10]}...'".encode('utf-8')
            yield audio_chunk
        else: # This 'else' block executes only if the loop completed without a 'break'
            logging.info(f"TTSPipeline {self.worker_id}: Finished generation for socket {socket_id} normally.")

# --- 2. Worker Thread Function ---
# This function is the target for each worker thread.
# It is designed to be persistent and only exit on global shutdown.
def _worker_run_loop(worker_id: str,
                     request_queue: queue.Queue,
                     response_queue: queue.Queue,
                     per_worker_stop_event: threading.Event,
                     worker_barrier: threading.Barrier,
                     socket_manager_instance): # Pass SocketManager instance for direct access

    tts_pipeline = TTSPipeline(worker_id)
    worker_barrier.wait() # Signal initialization complete to the main thread

    logging.info(f"Worker {worker_id}: Ready to process tasks.")

    # This loop keeps the worker thread alive and continuously processing tasks
    # It only breaks when the per_worker_stop_event is set (for application shutdown).
    while not per_worker_stop_event.is_set():
        try:
            # Get task from queue: (socket_id, request_data_dict, task_cancellation_event)
            socket_id, request_data, task_cancellation_event = request_queue.get(timeout=0.1)
        except queue.Empty:
            continue # No tasks currently, just wait

        # Pre-check: If the socket is already globally disconnected/cancelled, skip this task.
        # The worker remains alive.
        if socket_manager_instance.is_socket_globally_cancelled(socket_id):
            logging.info(f"Worker {worker_id}: Skipping task for globally disconnected socket {socket_id}.")
            request_queue.task_done()
            socket_manager_instance.remove_active_cancellation_event(socket_id, task_cancellation_event)
            continue # Move to the next task in the queue

        logging.info(f"Worker {worker_id}: Processing task for socket {socket_id}.")
        try:
            # Pass the task-specific cancellation event to the TTS pipeline
            for audio_chunk in tts_pipeline.generate_audio_chunks(request_data, task_cancellation_event):
                # This check ensures that if cancellation happens *during* chunk generation,
                # the worker stops sending data for this specific task.
                if task_cancellation_event.is_set():
                    logging.warning(f"Worker {worker_id}: Task for socket {socket_id} was cancelled during chunking. Discarding subsequent chunks.")
                    break # Break from the 'for' loop (generating chunks), NOT the worker's 'while' loop.
                response_queue.put((socket_id, audio_chunk))
            
        except Exception as e:
            logging.error(f"Worker {worker_id}: Error processing task for socket {socket_id}: {e}", exc_info=True)
            response_queue.put((socket_id, f"ERROR: {e}".encode('utf-8')))
        finally:
            # Always mark the task as done, regardless of success, cancellation, or error.
            request_queue.task_done()
            # Crucial: Remove the event from SocketManager's tracking as this task is now complete/cancelled.
            socket_manager_instance.remove_active_cancellation_event(socket_id, task_cancellation_event)

    logging.info(f"Worker {worker_id}: Exiting due to global stop signal.")

# --- 3. SocketManager ---
# Manages active sockets and their associated task cancellation events.
class SocketManager:
    def __init__(self):
        # Tracks general socket connection status
        # {socket_id: {'connected': True/False, 'user_id': str}}
        self.socket_status = {}
        self.socket_status_lock = threading.Lock()

        # Queues for inter-thread communication with workers
        self.request_queue = queue.Queue()
        self.response_queue = queue.Queue()

        # Stores lists of cancellation events for tasks currently active/in queue for each socket.
        # {socket_id: [event_for_task1, event_for_task2, ...]}
        self.active_socket_cancellation_events = {}
        self.events_lock = threading.Lock() # Lock for this dictionary

    def register_socket(self, socket_id: str, user_id: str):
        """Registers a new socket connection."""
        with self.socket_status_lock:
            self.socket_status[socket_id] = {'connected': True, 'user_id': user_id}
            logging.info(f"SocketManager: Registered socket {socket_id} for user {user_id}.")

    def unregister_socket(self, socket_id: str):
        """
        Marks a socket as disconnected and triggers cancellation for all its active tasks.
        This is typically called when a client disconnects.
        """
        logging.info(f"SocketManager: Unregistering socket {socket_id}.")
        
        # 1. Mark socket as disconnected in general status
        with self.socket_status_lock:
            if socket_id in self.socket_status:
                self.socket_status[socket_id]['connected'] = False
                logging.info(f"SocketManager: Socket {socket_id} marked as disconnected.")
            else:
                logging.warning(f"SocketManager: Attempted to unregister unknown socket {socket_id}.")
                return

        # 2. Trigger cancellation for all active tasks associated with this socket
        with self.events_lock:
            if socket_id in self.active_socket_cancellation_events:
                # Retrieve and remove all events for this socket
                events_for_socket = self.active_socket_cancellation_events.pop(socket_id)
                for event in events_for_socket:
                    event.set() # Set each event to signal cancellation
                logging.info(f"SocketManager: Triggered cancellation for {len(events_for_socket)} tasks from socket {socket_id}.")
            else:
                logging.debug(f"SocketManager: No active tasks found to cancel for socket {socket_id}.")

    def is_socket_globally_cancelled(self, socket_id: str) -> bool:
        """
        Checks if a socket is globally marked as disconnected/cancelled.
        Used by workers to skip tasks for already disconnected clients.
        """
        with self.socket_status_lock:
            status = self.socket_status.get(socket_id)
            return status is None or not status.get('connected', False)

    def add_task_for_socket(self, socket_id: str, text_content: str):
        """
        Adds a new TTS task for a specific socket.
        Creates and associates a unique cancellation event for this task.
        """
        # Pre-check: Don't enqueue if the socket is already known to be disconnected.
        if self.is_socket_globally_cancelled(socket_id):
            logging.warning(f"SocketManager: Not adding task for disconnected socket {socket_id}.")
            return

        # Create a brand-new cancellation event for THIS specific task
        task_cancellation_event = threading.Event()

        with self.events_lock:
            # Store the event in a list associated with the socket_id.
            # This allows multiple concurrent tasks per socket.
            self.active_socket_cancellation_events.setdefault(socket_id, []).append(task_cancellation_event)
            logging.debug(f"SocketManager: Added new cancellation event for socket {socket_id}. Total active for socket: {len(self.active_socket_cancellation_events[socket_id])}")

        task_payload = {'socket_id': socket_id, 'data': text_content}
        # Enqueue the task data along with its specific cancellation event
        self.request_queue.put((socket_id, task_payload, task_cancellation_event))
        logging.info(f"SocketManager: Enqueued task for socket {socket_id} (text: '{text_content[:20]}').")

    def remove_active_cancellation_event(self, socket_id: str, event_to_remove: threading.Event):
        """
        Removes a task's cancellation event from tracking after the task is processed,
        skipped, or explicitly cancelled. Called by workers.
        """
        with self.events_lock:
            if socket_id in self.active_socket_cancellation_events:
                try:
                    self.active_socket_cancellation_events[socket_id].remove(event_to_remove)
                    # Clean up the list if it becomes empty
                    if not self.active_socket_cancellation_events[socket_id]:
                        del self.active_socket_cancellation_events[socket_id]
                    logging.debug(f"SocketManager: Removed cancellation event for socket {socket_id}. Remaining: {len(self.active_socket_cancellation_events.get(socket_id, []))}")
                except ValueError:
                    # This can happen if the event was already removed (e.g., by unregister_socket)
                    logging.warning(f"SocketManager: Attempted to remove non-existent event for socket {socket_id}. Likely already handled.")

# --- 4. WorkerPoolManager ---
# Orchestrates the worker threads.
class WorkerPoolManager:
    def __init__(self, num_workers: int, socket_manager_instance: SocketManager):
        self.num_workers = num_workers
        # Stores {worker_id: {'thread': Thread_obj, 'stop_event': Event_obj}} for global worker control
        self.workers = {}
        self.socket_manager = socket_manager_instance # Reference to the shared SocketManager
        # Barrier to ensure all workers are initialized before main thread proceeds
        self.worker_barrier = threading.Barrier(self.num_workers + 1) # +1 for the main thread

    def start_workers(self):
        """Starts all worker threads in the pool."""
        logging.info(f"WorkerPoolManager: Starting {self.num_workers} worker threads.")
        for _ in range(self.num_workers):
            worker_id = str(uuid.uuid4())[:8] # Unique ID for each worker
            # Each worker gets its own 'stop_event' for graceful shutdown of THAT worker.
            per_worker_stop_event = threading.Event()

            worker_thread = threading.Thread(
                target=_worker_run_loop, # The function each thread will execute
                args=(worker_id, self.socket_manager.request_queue, self.socket_manager.response_queue,
                      per_worker_stop_event, self.worker_barrier, self.socket_manager),
                name=f"Worker-{worker_id}",
                daemon=True # Make threads daemons so they exit when the main program exits
            )
            self.workers[worker_id] = {
                'thread': worker_thread,
                'stop_event': per_worker_stop_event
            }
            worker_thread.start()
            logging.info(f"WorkerPoolManager: Started worker thread with ID: {worker_id}")

        self.worker_barrier.wait() # Wait for all workers to signal they've passed their barrier (initialized)
        logging.info("WorkerPoolManager: All worker threads initialized and passed the barrier.")

    def stop_all_workers(self):
        """Signals all workers to stop and waits for them to terminate gracefully."""
        logging.info("WorkerPoolManager: Signaling all worker threads to stop...")
        for worker_id, worker_info in self.workers.items():
            worker_info['stop_event'].set() # Set each worker's individual stop event
            logging.info(f"WorkerPoolManager: Set stop_event for worker {worker_id}.")

        # Wait for all worker threads to actually finish their loops
        for worker_id, worker_info in self.workers.items():
            if worker_info['thread'].is_alive(): # Only try to join if still running
                worker_info['thread'].join(timeout=2) # Wait for thread to finish, with a timeout
                if worker_info['thread'].is_alive():
                    logging.warning(f"WorkerPoolManager: Worker {worker_id} did not stop gracefully within timeout.")
                else:
                    logging.info(f"WorkerPoolManager: Worker {worker_id} stopped.")
        logging.info("WorkerPoolManager: All worker pool threads stopped.")


# --- Main Application Execution ---
if __name__ == "__main__":
    logging.info("--- Application Starting ---")

    # 1. Instantiate the SocketManager (handles queues and socket-specific events)
    socket_manager = SocketManager()

    # 2. Instantiate the WorkerPoolManager (orchestrates worker threads)
    num_tts_workers = 3
    worker_pool = WorkerPoolManager(num_tts_workers, socket_manager)

    # 3. Start the worker threads
    worker_pool.start_workers()
    time.sleep(0.1) # Give workers a moment to fully initialize

    # --- Simulate Client Interactions ---
    logging.info("\n--- Simulating client connections and adding TTS tasks ---")
    socket_id_alpha = "Client_Alpha_123"
    socket_id_beta = "Client_Beta_456"
    socket_id_gamma = "Client_Gamma_789"

    # Register clients
    socket_manager.register_socket(socket_id_alpha, "UserA")
    socket_manager.register_socket(socket_id_beta, "UserB")
    socket_manager.register_socket(socket_id_gamma, "UserC")

    # Add some initial tasks
    socket_manager.add_task_for_socket(socket_id_alpha, "This is a very long text segment for Client Alpha, intended to be interrupted mid-way.")
    socket_manager.add_task_for_socket(socket_id_beta, "Short text for Client Beta, should complete normally.")
    socket_manager.add_task_for_socket(socket_id_alpha, "Another long text for Client Alpha, also likely to be interrupted.")
    socket_manager.add_task_for_socket(socket_id_gamma, "Regular text for Client Gamma.")

    time.sleep(0.3) # Allow workers to pick up and start processing tasks

    # --- Simulate Client Alpha disconnecting ---
    logging.info(f"\n--- Simulating {socket_id_alpha} DISCONNECTING ---")
    socket_manager.unregister_socket(socket_id_alpha) # This will trigger cancellation for all tasks from Client Alpha

    time.sleep(1.0) # Give time for cancellation to propagate and other tasks to progress

    # --- Test adding tasks for disconnected vs. active clients ---
    logging.info(f"\n--- Testing adding tasks after disconnection ---")
    socket_manager.add_task_for_socket(socket_id_beta, "Another task for Client Beta, should process fine.")
    socket_manager.add_task_for_socket(socket_id_alpha, "This task for Client Alpha should NOT be added (socket already unregistered).")
    socket_manager.add_task_for_socket(socket_id_gamma, "Final task for Client Gamma.")

    logging.info("\n--- Waiting for all enqueued tasks to be processed/skipped ---")
    socket_manager.request_queue.join() # Blocks until all items in the request_queue are processed.
    logging.info("All tasks processed or skipped.")

    # --- Simulate consuming responses ---
    logging.info("\n--- Consuming responses from the response queue (simulated) ---")
    while not socket_manager.response_queue.empty():
        sock_id, response_data = socket_manager.response_queue.get()
        logging.info(f"Response received for {sock_id}: {response_data.decode('utf-8')[:30]}...")
        socket_manager.response_queue.task_done()
    logging.info("All responses consumed.")

    # --- Application Shutdown ---
    logging.info("\n--- Shutting down worker pool ---")
    worker_pool.stop_all_workers()
    logging.info("--- Application Finished ---")